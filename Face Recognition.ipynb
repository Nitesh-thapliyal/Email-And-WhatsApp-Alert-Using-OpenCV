{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:15: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-1-140b455248b7>:15: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load HAAR face classifier\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load functions\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "# Collect 100 samples of your face from webcam input\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count += 1\n",
    "        face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Save file in specified directory with unique name\n",
    "        #path\n",
    "        file_name_path = 'G:/Summer Program/Tasks/Task6-Alert-System/My_Image_Dataset/' + str(count) + '.jpg'\n",
    "        cv2.imwrite(file_name_path, face)\n",
    "\n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "        \n",
    "    else:\n",
    "        print(\"Face not found\")\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13 or count == 100: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained sucessefully\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Get the training data we previously made\n",
    "data_path_1 = 'G:/Summer Program/Tasks/Task6-Alert-System/My_Image_Dataset/'\n",
    "onlyfiles_1 = [f for f in listdir(data_path_1) if isfile(join(data_path_1, f))]\n",
    "\n",
    "\n",
    "# Create arrays for training data and labels\n",
    "Training_Data_1, Labels_1 = [], []\n",
    "# Create arrays for training data and labels\n",
    "\n",
    "# Create a numpy array for training dataset 1\n",
    "for i, files in enumerate(onlyfiles_1):\n",
    "    image_path = data_path_1 + onlyfiles_1[i]\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    Training_Data_1.append(np.asarray(images, dtype=np.uint8))\n",
    "    Labels_1.append(i)   \n",
    "\n",
    "# Create a numpy array for both training data and labels for person 1\n",
    "Labels_1 = np.asarray(Labels_1, dtype=np.int32)\n",
    "\n",
    "# Initialize facial recognizer\n",
    "#model = cv2.face.createLBPHFaceRecognizer()\n",
    "# NOTE: For OpenCV 3.0 use cv2.face.createLBPHFaceRecognizer()\n",
    "# pip install opencv-contrib-python\n",
    "#model = cv2.createLBPHFaceRecognizer()\n",
    "\n",
    "Nitesh_model  = cv2.face_LBPHFaceRecognizer.create()\n",
    "# Let's train our model \n",
    "Nitesh_model.train(np.asarray(Training_Data_1), np.asarray(Labels_1))\n",
    "print(\"Model trained sucessefully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lauch AWS instance using Terraform when face is recognised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:13: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-3-7ac0f8374995>:13: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n",
      "'#terraform' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1maws_instance.os1: Refreshing state... [id=i-07c3d200a11ce50e0]\u001b[0m\n",
      "\u001b[0m\u001b[1maws_ebs_volume.st1: Refreshing state... [id=vol-0e4577c488a197d1f]\u001b[0m\n",
      "\u001b[0m\u001b[1maws_volume_attachment.ebs_att: Refreshing state... [id=vai-991301012]\u001b[0m\n",
      "\u001b[0m\u001b[1maws_ebs_volume.st1: Destroying... [id=vol-0e4577c488a197d1f]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1maws_ebs_volume.st1: Destruction complete after 0s\u001b[0m\n",
      "\u001b[0m\u001b[1maws_instance.os1: Creating...\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1maws_instance.os1: Still creating... [10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1maws_instance.os1: Still creating... [20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1maws_instance.os1: Still creating... [30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1maws_instance.os1: Creation complete after 33s [id=i-0d1e951772b6e049d]\u001b[0m\n",
      "\u001b[0m\u001b[1maws_ebs_volume.st1: Creating...\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1maws_ebs_volume.st1: Still creating... [10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1maws_ebs_volume.st1: Creation complete after 11s [id=vol-06c89d52fc685bcea]\u001b[0m\n",
      "\u001b[0m\u001b[1maws_volume_attachment.ebs_att: Creating...\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1maws_volume_attachment.ebs_att: Still creating... [10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1maws_volume_attachment.ebs_att: Still creating... [20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1maws_volume_attachment.ebs_att: Creation complete after 21s [id=vai-977186061]\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[32m\n",
      "Apply complete! Resources: 3 added, 0 changed, 1 destroyed.\n",
      "\u001b[0m\u001b[0m\u001b[1m\u001b[32m\n",
      "Outputs:\n",
      "\n",
      "\u001b[0mos1 = {\n",
      "  \"ami\" = \"ami-010aff33ed5991201\"\n",
      "  \"arn\" = \"arn:aws:ec2:ap-south-1:264241028732:instance/i-0d1e951772b6e049d\"\n",
      "  \"associate_public_ip_address\" = true\n",
      "  \"availability_zone\" = \"ap-south-1a\"\n",
      "  \"capacity_reservation_specification\" = tolist([\n",
      "    {\n",
      "      \"capacity_reservation_preference\" = \"open\"\n",
      "      \"capacity_reservation_target\" = tolist([])\n",
      "    },\n",
      "  ])\n",
      "  \"cpu_core_count\" = 1\n",
      "  \"cpu_threads_per_core\" = 1\n",
      "  \"credit_specification\" = tolist([\n",
      "    {\n",
      "      \"cpu_credits\" = \"standard\"\n",
      "    },\n",
      "  ])\n",
      "  \"disable_api_termination\" = false\n",
      "  \"ebs_block_device\" = toset([])\n",
      "  \"ebs_optimized\" = false\n",
      "  \"enclave_options\" = tolist([\n",
      "    {\n",
      "      \"enabled\" = false\n",
      "    },\n",
      "  ])\n",
      "  \"ephemeral_block_device\" = toset([])\n",
      "  \"get_password_data\" = false\n",
      "  \"hibernation\" = false\n",
      "  \"host_id\" = tostring(null)\n",
      "  \"iam_instance_profile\" = \"\"\n",
      "  \"id\" = \"i-0d1e951772b6e049d\"\n",
      "  \"instance_initiated_shutdown_behavior\" = \"stop\"\n",
      "  \"instance_state\" = \"running\"\n",
      "  \"instance_type\" = \"t2.micro\"\n",
      "  \"ipv6_address_count\" = 0\n",
      "  \"ipv6_addresses\" = tolist([])\n",
      "  \"key_name\" = \"\"\n",
      "  \"metadata_options\" = tolist([\n",
      "    {\n",
      "      \"http_endpoint\" = \"enabled\"\n",
      "      \"http_put_response_hop_limit\" = 1\n",
      "      \"http_tokens\" = \"optional\"\n",
      "    },\n",
      "  ])\n",
      "  \"monitoring\" = false\n",
      "  \"network_interface\" = toset([])\n",
      "  \"outpost_arn\" = \"\"\n",
      "  \"password_data\" = \"\"\n",
      "  \"placement_group\" = \"\"\n",
      "  \"primary_network_interface_id\" = \"eni-0687a4645c0edc79e\"\n",
      "  \"private_dns\" = \"ip-172-31-34-217.ap-south-1.compute.internal\"\n",
      "  \"private_ip\" = \"172.31.34.217\"\n",
      "  \"public_dns\" = \"ec2-13-233-125-254.ap-south-1.compute.amazonaws.com\"\n",
      "  \"public_ip\" = \"13.233.125.254\"\n",
      "  \"root_block_device\" = tolist([\n",
      "    {\n",
      "      \"delete_on_termination\" = true\n",
      "      \"device_name\" = \"/dev/xvda\"\n",
      "      \"encrypted\" = false\n",
      "      \"iops\" = 100\n",
      "      \"kms_key_id\" = \"\"\n",
      "      \"tags\" = tomap({})\n",
      "      \"throughput\" = 0\n",
      "      \"volume_id\" = \"vol-065b64e07a781e4b8\"\n",
      "      \"volume_size\" = 8\n",
      "      \"volume_type\" = \"gp2\"\n",
      "    },\n",
      "  ])\n",
      "  \"secondary_private_ips\" = toset([])\n",
      "  \"security_groups\" = toset([\n",
      "    \"Terraform\",\n",
      "  ])\n",
      "  \"source_dest_check\" = true\n",
      "  \"subnet_id\" = \"subnet-31595659\"\n",
      "  \"tags\" = tomap({\n",
      "    \"Name\" = \"Terraform-OS\"\n",
      "  })\n",
      "  \"tags_all\" = tomap({\n",
      "    \"Name\" = \"Terraform-OS\"\n",
      "  })\n",
      "  \"tenancy\" = \"default\"\n",
      "  \"timeouts\" = null /* object */\n",
      "  \"user_data\" = tostring(null)\n",
      "  \"user_data_base64\" = tostring(null)\n",
      "  \"volume_tags\" = tomap(null) /* of string */\n",
      "  \"vpc_security_group_ids\" = toset([\n",
      "    \"sg-03f830056e627513d\",\n",
      "  ])\n",
      "}\n",
      "os2 = \"13.233.125.254\"\n",
      "os3 = \"ap-south-1a\"\n",
      "os4 = {\n",
      "  \"arn\" = \"arn:aws:ec2:ap-south-1:264241028732:volume/vol-06c89d52fc685bcea\"\n",
      "  \"availability_zone\" = \"ap-south-1a\"\n",
      "  \"encrypted\" = false\n",
      "  \"id\" = \"vol-06c89d52fc685bcea\"\n",
      "  \"iops\" = 100\n",
      "  \"kms_key_id\" = \"\"\n",
      "  \"multi_attach_enabled\" = false\n",
      "  \"outpost_arn\" = \"\"\n",
      "  \"size\" = 5\n",
      "  \"snapshot_id\" = \"\"\n",
      "  \"tags\" = tomap({\n",
      "    \"name\" = \" Nitesh new Volume\"\n",
      "  })\n",
      "  \"tags_all\" = tomap({\n",
      "    \"name\" = \" Nitesh new Volume\"\n",
      "  })\n",
      "  \"throughput\" = 0\n",
      "  \"type\" = \"gp2\"\n",
      "}\n",
      "os5 = {\n",
      "  \"device_name\" = \"/dev/sdh\"\n",
      "  \"force_detach\" = tobool(null)\n",
      "  \"id\" = \"vai-977186061\"\n",
      "  \"instance_id\" = \"i-0d1e951772b6e049d\"\n",
      "  \"skip_destroy\" = tobool(null)\n",
      "  \"volume_id\" = \"vol-06c89d52fc685bcea\"\n",
      "}\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.2) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-k1ohfcms\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7ac0f8374995>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7ac0f8374995>\u001b[0m in \u001b[0;36mface_detector\u001b[1;34m(img, size)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Convert image to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfaces\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.2) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-k1ohfcms\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img, size=0.5):\n",
    "    \n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return img, []\n",
    "    \n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))\n",
    "    return img, roi\n",
    "\n",
    "\n",
    "# Open Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    image, face = face_detector(frame)\n",
    "    \n",
    "    try:\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Pass face to prediction model\n",
    "        # \"results\" comprises of a tuple containing the label and the confidence value\n",
    "        results = Nitesh_model.predict(face)\n",
    "        \n",
    "        if results[1] < 500:\n",
    "            confidence = int( 100 * (1 - (results[1])/400) )\n",
    "            display_string = str(confidence) + '% Confident it is User'\n",
    "            \n",
    "        cv2.putText(image, display_string, (100, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (255,120,150), 2)\n",
    "        \n",
    "        if confidence > 70:\n",
    "            cv2.putText(image, \"Hello Nitesh\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow('Face Recognitioned', image )\n",
    "            if cv2.waitKey(1)==13:\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                !terraform init\n",
    "                !terraform apply --auto-approve\n",
    "                \n",
    "         \n",
    "        else:\n",
    "            cv2.putText(image, \"Unrecognised Face\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "            \n",
    "    except:\n",
    "        cv2.putText(image, \"Face Not Found\", (220, 120) , cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.putText(image, \"Searching for Face....\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.imshow('Face Recognition', image )\n",
    "        pass\n",
    "        \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
